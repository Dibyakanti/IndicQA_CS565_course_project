{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport time, sys","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast,RobertaForQuestionAnswering,XLMRobertaForQuestionAnswering,XLMRobertaTokenizerFast,BertTokenizerFast,BertForQuestionAnswering\n\ntokenizer = XLMRobertaTokenizerFast.from_pretrained('../input/huggingface-question-answering-models/multilingual/xlm-roberta-large-squad2')\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ntrain_data = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\")\ndev_data = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/test.csv\")\n# size = 100 # size if train set used\n\ndef preprocess_data(data):\n    encodings = tokenizer(list(data[\"context\"]),list(data[\"question\"]), truncation=True, padding=True)\n    \n    start_positions = []\n    end_positions = []\n    for i in range(len(data[\"answer_start\"])):\n        start_positions.append(encodings.char_to_token(i,data[\"answer_start\"][i]))\n        end_positions.append(encodings.char_to_token( i, (data[\"answer_start\"][i] + len(data['answer_text'][i])-1) ))\n        \n            \n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n        \n        \n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n    \n    return encodings\n\nlen(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = preprocess_data(train_data)\ndev_encodings = tokenizer(list(dev_data[\"context\"]), list(dev_data[\"question\"]), truncation=True, padding=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\n\nclass chaiDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n    \ntrain_dataset = chaiDataset(train_encodings)\ndev_dataset = chaiDataset(dev_encodings)\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XLMRobertaForQuestionAnswering.from_pretrained('../input/huggingface-question-answering-models/multilingual/xlm-roberta-large-squad2')\n\nfor param in model.roberta.parameters():\n    param.requires_grad = False\n\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = AdamW(model.parameters(), lr=1e-4)\n\nmodel.train()\n\nfor epoch in range(40):\n    epoch_loss=0\n    for batch in train_loader:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        batch_loss=loss.item()\n        optim.step()\n        epoch_loss+=batch_loss\n    normalized_epoch_loss = epoch_loss/(len(train_loader))\n    print(\"Epoch {} ; Epoch loss: {} \".format(epoch+1,normalized_epoch_loss))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,\"model_2\")\n# model = torch.load(\"../input/chaii-torch/model_2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\noutput_words,output_id = [],[]\nfor batch in dev_loader:\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask)\n    start = torch.argmax(outputs[\"start_logits\"])\n    end = torch.argmax(outputs[\"end_logits\"])\n    output_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start:end+1])\n    output_words.append(tokenizer.convert_tokens_to_string(output_tokens))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_results = pd.DataFrame({\"id\":dev_data[\"id\"],\"PredictionString\":output_words})\ndev_results.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}